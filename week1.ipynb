{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下周三晚上 0 5 TIME\n",
      "第二 6 8 ORDINAL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "text = \"下周三晚上给第二个Keynote定稿\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名实体识别 - 模式匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "订单号:  202303251200ABC\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'^\\d{12}[A-Z]{3}$' # 这个正则表达式匹配12个数字后跟3个大写字母\n",
    "\n",
    "text = \"你好，我有一个订单一直没有收到，订单号是202303251200ABC\"\n",
    "doc = nlp(text)\n",
    "for word in doc:\n",
    "    match = re.search(pattern, word.text)\n",
    "    if match:\n",
    "        print(\"订单号: \", match.group())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名实体识别 - 直接通过ChatGPT的API来做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。\n",
      "\n",
      "\"orderNumber\": \"202303251200ABC\"\n"
     ]
    }
   ],
   "source": [
    "import openai, os\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "COMPLETION_MODEL = \"text-davinci-003\"\n",
    "def get_response(prompt, temperature = 1.0, stop=None):\n",
    "    completions = openai.Completion.create(\n",
    "        engine=COMPLETION_MODEL,\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=stop,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    message = completions['choices'][0]['text']\n",
    "    return message\n",
    "\n",
    "text = \"你好，我有一个订单一直没有收到，订单号是202303251200ABC\"\n",
    "\n",
    "print(get_response(\"请从下面的文本中提取出用户的订单号，并以json形式显示：\\n\\n\" + text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词找一些有用的关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大气\n",
      "满意\n",
      "好\n",
      "流畅\n"
     ]
    }
   ],
   "source": [
    "text = '''外形外观：紫色，非常大气，方方正正，四周圆角\n",
    "屏幕音效：清晰度分辨率都挺满意，听音乐效果也不错\n",
    "拍照效果：非常好\n",
    "运行速度：很流畅\n",
    "待机时间：之前的安卓手机用了近三年，一天3充，现在妥妥的一天一充'''\n",
    "\n",
    "useful_words_list = ['大气', '满意', '好', '流畅']\n",
    "\n",
    "doc = nlp(text)\n",
    "for word in doc:\n",
    "    if word.text in useful_words_list:\n",
    "        print(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算 N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('外形', '外观'),\n",
       " ('外观', '：'),\n",
       " ('：', '紫色'),\n",
       " ('紫色', '，'),\n",
       " ('，', '非常'),\n",
       " ('非常', '大气'),\n",
       " ('大气', '，'),\n",
       " ('，', '方方正正'),\n",
       " ('方方正正', '，'),\n",
       " ('，', '四周'),\n",
       " ('四周', '圆角'),\n",
       " ('圆角', '\\n'),\n",
       " ('\\n', '屏幕'),\n",
       " ('屏幕', '音效'),\n",
       " ('音效', '：'),\n",
       " ('：', '清晰度'),\n",
       " ('清晰度', '分辨率'),\n",
       " ('分辨率', '都'),\n",
       " ('都', '挺'),\n",
       " ('挺', '满意'),\n",
       " ('满意', '，'),\n",
       " ('，', '听'),\n",
       " ('听', '音乐'),\n",
       " ('音乐', '效果'),\n",
       " ('效果', '也'),\n",
       " ('也', '不错'),\n",
       " ('不错', '\\n'),\n",
       " ('\\n', '拍照'),\n",
       " ('拍照', '效果'),\n",
       " ('效果', '：'),\n",
       " ('：', '非常'),\n",
       " ('非常', '好'),\n",
       " ('好', '\\n'),\n",
       " ('\\n', '运行'),\n",
       " ('运行', '速度'),\n",
       " ('速度', '：'),\n",
       " ('：', '很'),\n",
       " ('很', '流畅'),\n",
       " ('流畅', '\\n'),\n",
       " ('\\n', '待机'),\n",
       " ('待机', '时间'),\n",
       " ('时间', '：'),\n",
       " ('：', '之前'),\n",
       " ('之前', '的'),\n",
       " ('的', '安卓'),\n",
       " ('安卓', '手机'),\n",
       " ('手机', '用'),\n",
       " ('用', '了'),\n",
       " ('了', '近'),\n",
       " ('近', '三年'),\n",
       " ('三年', '，'),\n",
       " ('，', '一'),\n",
       " ('一', '天'),\n",
       " ('天', '3'),\n",
       " ('3', '充'),\n",
       " ('充', '，'),\n",
       " ('，', '现在'),\n",
       " ('现在', '妥妥'),\n",
       " ('妥妥', '的'),\n",
       " ('的', '一'),\n",
       " ('一', '天一'),\n",
       " ('天一', '充')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''外形外观：紫色，非常大气，方方正正，四周圆角\n",
    "屏幕音效：清晰度分辨率都挺满意，听音乐效果也不错\n",
    "拍照效果：非常好\n",
    "运行速度：很流畅\n",
    "待机时间：之前的安卓手机用了近三年，一天3充，现在妥妥的一天一充'''\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "words = []\n",
    "for word in doc:\n",
    "    words.append(word.text)\n",
    "\n",
    "list(ngrams(words, 2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'is', 'first', 'this', 'second', 'document'}\n",
      "{'the': 0, 'is': 1, 'first': 1, 'this': 1, 'second': 0, 'document': 2}\n",
      "{'the': 1, 'is': 1, 'first': 0, 'this': 1, 'second': 1, 'document': 2}\n"
     ]
    }
   ],
   "source": [
    "doc_a = 'this document is first document'\n",
    "doc_b = 'this document is the second document'\n",
    "\n",
    "bag_of_words_a = doc_a.split(' ')\n",
    "bag_of_words_b = doc_b.split(' ')\n",
    "\n",
    "unique_words_set = set(bag_of_words_a).union(set(bag_of_words_b))\n",
    "print(unique_words_set)\n",
    "\n",
    "# Now create a dictionary of words and their occurence for each document in the corpus (collection of documents).\n",
    "\n",
    "dict_a = dict.fromkeys(unique_words_set, 0)\n",
    "# print(dict_a) # {'this': 0, 'document': 0, 'second': 0, 'is': 0, 'the': 0}\n",
    "\n",
    "for word in bag_of_words_a:\n",
    "    dict_a[word] += 1\n",
    "\n",
    "print(dict_a)\n",
    "# {'this': 1, 'document': 2, 'second': 1, 'is': 1, 'the': 1}\n",
    "\n",
    "# similarly\n",
    "\n",
    "dict_b = dict.fromkeys(unique_words_set, 0)\n",
    "\n",
    "for word in bag_of_words_b:\n",
    "    dict_b[word] += 1\n",
    "\n",
    "print(dict_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0.0, 'is': 0.2, 'first': 0.2, 'this': 0.2, 'second': 0.0, 'document': 0.4}\n"
     ]
    }
   ],
   "source": [
    "def compute_term_frequency(word_dictionary, bag_of_words):\n",
    "    term_frequency_dictionary = {}\n",
    "    length_of_bag_of_words = len(bag_of_words)\n",
    "\n",
    "    for word, count in word_dictionary.items():\n",
    "        term_frequency_dictionary[word] = count / float(length_of_bag_of_words)\n",
    "\n",
    "    return term_frequency_dictionary\n",
    "\n",
    "# Implementation\n",
    "\n",
    "print(compute_term_frequency(dict_a, bag_of_words_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"the\": 0.4054651081081644,\n",
      "  \"is\": 0.0,\n",
      "  \"first\": 0.4054651081081644,\n",
      "  \"this\": 0.0,\n",
      "  \"second\": 0.4054651081081644,\n",
      "  \"document\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_inverse_document_frequency(full_doc_list):\n",
    "    idf_dict = {}\n",
    "    length_of_doc_list = len(full_doc_list)\n",
    "\n",
    "    # Initialize dictionary with words from all documents\n",
    "    idf_dict = dict.fromkeys(full_doc_list[0].keys(), 0)\n",
    "    \n",
    "    # Count the number of documents that contain each word\n",
    "    for doc in full_doc_list:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idf_dict[word] += 1\n",
    "\n",
    "    # Compute IDF for each word\n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log((length_of_doc_list+1) / (float(val) + 1))\n",
    "\n",
    "    return idf_dict\n",
    "\n",
    "final_idf_dict = compute_inverse_document_frequency([dict_a, dict_b])\n",
    "\n",
    "import json\n",
    "\n",
    "# Print the final_idf_dict dictionary in a pretty format\n",
    "print(json.dumps(final_idf_dict, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接使用TF/IDF的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n",
    "\n",
    "df = pd.read_csv(datafile_path)\n",
    "df['embedding'] = df.embedding.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Unnamed: 0   ProductId          UserId  Score   \n",
       "0             0  B003XPF9BO  A3R7JR3FMEBXQB      5  \\\n",
       "1           297  B003VXHGPK  A21VWSCGW7UUAR      4   \n",
       "2           296  B008JKTTUA  A34XBAIFT02B60      1   \n",
       "3           295  B000LKTTTW  A14MQ40CCU8B13      5   \n",
       "4           294  B001D09KAM  A34XBAIFT02B60      1   \n",
       "..          ...         ...             ...    ...   \n",
       "995         623  B0000CFXYA  A3GS4GWPIBV0NT      1   \n",
       "996         624  B0001BH5YM   A1BZ3HMAKK0NC      5   \n",
       "997         625  B0009ET7TC  A2FSDQY5AI6TNX      5   \n",
       "998         619  B007PA32L2  A15FF2P7RPKH6G      5   \n",
       "999         999  B001EQ5GEO  A3VYU0VO6DYV6I      5   \n",
       "\n",
       "                                               Summary   \n",
       "0    where does one  start...and stop... with a tre...  \\\n",
       "1                     Good, but not Wolfgang Puck good   \n",
       "2    Should advertise coconut as an ingredient more...   \n",
       "3                                     Best tomato soup   \n",
       "4    Should advertise coconut as an ingredient more...   \n",
       "..                                                 ...   \n",
       "995                      Strange inflammation response   \n",
       "996                      My favorite and only  MUSTARD   \n",
       "997                           My furbabies LOVE these!   \n",
       "998                          got this for the daughter   \n",
       "999                                I love Maui Coffee!   \n",
       "\n",
       "                                                  Text   \n",
       "0    Wanted to save some to bring to my Chicago fam...  \\\n",
       "1    Honestly, I have to admit that I expected a li...   \n",
       "2    First, these should be called Mac - Coconut ba...   \n",
       "3    I have a hard time finding packaged food of an...   \n",
       "4    First, these should be called Mac - Coconut ba...   \n",
       "..                                                 ...   \n",
       "995  Truthfully wasn't crazy about the taste of the...   \n",
       "996  You've just got to experience this mustard... ...   \n",
       "997  Shake the container and they come running. Eve...   \n",
       "998  all i have heard since she got a kuerig is why...   \n",
       "999  My first experience with Maui Coffee was bring...   \n",
       "\n",
       "                                              combined  n_tokens   \n",
       "0    Title: where does one  start...and stop... wit...        52  \\\n",
       "1    Title: Good, but not Wolfgang Puck good; Conte...       178   \n",
       "2    Title: Should advertise coconut as an ingredie...        78   \n",
       "3    Title: Best tomato soup; Content: I have a har...       111   \n",
       "4    Title: Should advertise coconut as an ingredie...        78   \n",
       "..                                                 ...       ...   \n",
       "995  Title: Strange inflammation response; Content:...       110   \n",
       "996  Title: My favorite and only  MUSTARD; Content:...        80   \n",
       "997  Title: My furbabies LOVE these!; Content: Shak...        47   \n",
       "998  Title: got this for the daughter; Content: all...        50   \n",
       "999  Title: I love Maui Coffee!; Content: My first ...       118   \n",
       "\n",
       "                                             embedding  \n",
       "0    [0.007018072064965963, -0.02731654793024063, 0...  \n",
       "1    [-0.003140551969408989, -0.009995664469897747,...  \n",
       "2    [-0.01757248118519783, -8.266511576948687e-05,...  \n",
       "3    [-0.0013932279543951154, -0.011112828738987446...  \n",
       "4    [-0.01757248118519783, -8.266511576948687e-05,...  \n",
       "..                                                 ...  \n",
       "995  [0.00011091353371739388, -0.00466986745595932,...  \n",
       "996  [-0.020869314670562744, -0.013138455338776112,...  \n",
       "997  [-0.009749102406203747, -0.0068712360225617886...  \n",
       "998  [-0.00521062919870019, 0.0009606690146028996, ...  \n",
       "999  [-0.006057822611182928, -0.015015840530395508,...  \n",
       "\n",
       "[1000 rows x 9 columns]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      12   13   21   31   32   34   40   50   51   60  ...  yogurt  yorkie   \n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0  \\\n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...     ...     ...   \n",
      "995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n",
      "\n",
      "     you  your  yuk  yum  yummmm  yummmmm  yummy  zipfizz  \n",
      "0    0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "1    0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "2    0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "3    0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "4    0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "..   ...   ...  ...  ...     ...      ...    ...      ...  \n",
      "995  0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "996  0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "997  0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "998  0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "999  0.0   0.0  0.0  0.0     0.0      0.0    0.0      0.0  \n",
      "\n",
      "[1000 rows x 1038 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = df['Summary']\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: [('start', 0.394384915946285), ('stop', 0.394384915946285), ('where', 0.394384915946285), ('does', 0.35649943107170623), ('treat', 0.312176255125172)]\n",
      "Row 2: [('good', 0.5681028016321258), ('puck', 0.49441945926150405), ('wolfgang', 0.49441945926150405), ('but', 0.3153408915839207), ('not', 0.2981914242226147)]\n",
      "Row 3: [('advertise', 0.36559695823357646), ('ingredient', 0.36559695823357646), ('prominently', 0.36559695823357646), ('should', 0.36559695823357646), ('an', 0.3574098136743376)]\n",
      "Row 4: [('tomato', 0.6610768832907321), ('soup', 0.6331508937690398), ('best', 0.4026130898245583), ('12', 0.0), ('13', 0.0)]\n",
      "Row 5: [('advertise', 0.36559695823357646), ('ingredient', 0.36559695823357646), ('prominently', 0.36559695823357646), ('should', 0.36559695823357646), ('an', 0.3574098136743376)]\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_words(row, tfidf_df, top_n=5):\n",
    "    # Get the words and corresponding tf-idf scores for the given row\n",
    "    words_and_scores = [(word, tfidf_df.loc[row, word]) for word in tfidf_df.columns]\n",
    "    \n",
    "    # Sort the words by their tf-idf scores\n",
    "    words_and_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top n words\n",
    "    top_n_words_and_scores = words_and_scores[:top_n]\n",
    "    \n",
    "    return top_n_words_and_scores\n",
    "\n",
    "# Print the highest tf-idf words and their scores for the first 5 rows\n",
    "for i in range(5):\n",
    "    print(f\"Row {i+1}: {get_top_n_words(i, tfidf_df)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec的演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mactalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
