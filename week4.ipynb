{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Fine-Tune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,openai,backoff\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "dynasties= ['唐', '宋', '元', '明', '清', '汉', '魏', '晋', '南北朝']\n",
    "super_powers = ['隐形', '飞行', '读心术', '瞬间移动', '不死之身', '喷火']\n",
    "story_types = ['轻松', '努力', '艰难']\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def gpt35(prompt, max_tokens=2048, temperature=0.5, top_p=1, frequency_penalty=0, presence_penalty=0):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty)\n",
    "    return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "def prepare_stories(dynasties, super_powers, story_types, output_file=\"data/ultraman_stories.csv\"):\n",
    "    df = pd.DataFrame()\n",
    "    repeat = 3\n",
    "    for dynasty in dynasties:\n",
    "        for super_power in super_powers:\n",
    "            for story_type in story_types:\n",
    "                   for i in range(repeat):\n",
    "                        prompt = f\"\"\"请你用中文写一段300字的故事，情节跌宕起伏，讲述一位{dynasty}朝时期的英雄人物，穿越到现代，拥有了{super_power}这样的超能力，通过{story_type}的战斗，帮助奥特曼一起打败了怪兽的故事。\"\"\"\n",
    "                        story = gpt35(prompt)\n",
    "                        row = {\"dynasty\": dynasty, \"super_power\": super_power, \"story_type\": story_type, \"story\": story}\n",
    "                        row = pd.DataFrame([row])\n",
    "                        df = pd.concat([df, row], axis=0, ignore_index=True)\n",
    "\n",
    "    df.to_csv(\"data/ultraman_stories.csv\")\n",
    "\n",
    "prepare_stories(dynasties, super_powers, story_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Based on your file extension, your file is formatted as a CSV file\n",
      "- Your file contains 464 prompt-completion pairs\n",
      "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
      "- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
      "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
      "- [Recommended] Add a suffix ending `.` to all completions [Y/n]: Y\n",
      "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified file to `data/prepared_data_mactalk_prepared.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"data/prepared_data_mactalk_prepared.jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 8.82 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['openai', 'tools', 'fine_tunes.prepare_data', '--file', 'data/prepared_data_mactalk.csv', '--quiet'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ultraman_stories.csv\")\n",
    "df['sub_prompt'] = df['dynasty'] + \",\" + df['super_power'] + \",\" + df['story_type']\n",
    "prepared_data = df.loc[:,['sub_prompt','story']]\n",
    "prepared_data.rename(columns={'sub_prompt':'prompt', 'story':'completion'}, inplace=True)\n",
    "prepared_data.to_csv('data/prepared_data_mactalk.csv',index=False)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run('openai tools fine_tunes.prepare_data --file data/prepared_data_mactalk.csv --quiet'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 446k/446k [00:00<00:00, 342Mit/s]\n",
      "\u001b[91mError:\u001b[0m HTTP code 500 from API (<html>\n",
      "  <head>\n",
      "    <title>Internal Server Error</title>\n",
      "  </head>\n",
      "  <body>\n",
      "    <h1><p>Internal Server Error</p></h1>\n",
      "    \n",
      "  </body>\n",
      "</html>\n",
      ") (HTTP status code: 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['openai', 'api', 'fine_tunes.create', '--training_file', 'data/prepared_data_mactalk_prepared.jsonl', '--model', 'curie', '--suffix', '\"mactalk_ultraman\"'], returncode=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run('openai api fine_tunes.create --training_file data/prepared_data_mactalk_prepared.jsonl --model curie --suffix \"mactalk_ultraman\"'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"created_at\": 1680576711,\n",
      "      \"fine_tuned_model\": \"curie:ft-bothub-ai:ultraman-2023-04-04-03-03-26\",\n",
      "      \"hyperparams\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"learning_rate_multiplier\": 0.2,\n",
      "        \"n_epochs\": 4,\n",
      "        \"prompt_loss_weight\": 0.01\n",
      "      },\n",
      "      \"id\": \"ft-3oxkr1zBVB4fJWogJDDjQbr0\",\n",
      "      \"model\": \"curie\",\n",
      "      \"object\": \"fine-tune\",\n",
      "      \"organization_id\": \"org-yQaCtAdY0voCWSs0QNqSLfda\",\n",
      "      \"result_files\": [\n",
      "        {\n",
      "          \"bytes\": 107785,\n",
      "          \"created_at\": 1680577408,\n",
      "          \"filename\": \"compiled_results.csv\",\n",
      "          \"id\": \"file-LuSjYlkMa6fHHRB23bnr3L1z\",\n",
      "          \"object\": \"file\",\n",
      "          \"purpose\": \"fine-tune-results\",\n",
      "          \"status\": \"processed\",\n",
      "          \"status_details\": null\n",
      "        }\n",
      "      ],\n",
      "      \"status\": \"succeeded\",\n",
      "      \"training_files\": [\n",
      "        {\n",
      "          \"bytes\": 446199,\n",
      "          \"created_at\": 1680576711,\n",
      "          \"filename\": \"data/prepared_data_prepared.jsonl\",\n",
      "          \"id\": \"file-yn0BfnPmgvf7n0sfQzQRbbeE\",\n",
      "          \"object\": \"file\",\n",
      "          \"purpose\": \"fine-tune\",\n",
      "          \"status\": \"processed\",\n",
      "          \"status_details\": null\n",
      "        }\n",
      "      ],\n",
      "      \"updated_at\": 1680577408,\n",
      "      \"validation_files\": []\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1680670677,\n",
      "      \"fine_tuned_model\": \"curie:ft-bothub-ai:ultraman-2023-04-05-05-01-39\",\n",
      "      \"hyperparams\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"learning_rate_multiplier\": 2.0,\n",
      "        \"n_epochs\": 4,\n",
      "        \"prompt_loss_weight\": 0.01\n",
      "      },\n",
      "      \"id\": \"ft-CFLSbHbKvg5BEspBy3aQHSbm\",\n",
      "      \"model\": \"curie:ft-bothub-ai:ultraman-2023-04-04-03-03-26\",\n",
      "      \"object\": \"fine-tune\",\n",
      "      \"organization_id\": \"org-yQaCtAdY0voCWSs0QNqSLfda\",\n",
      "      \"result_files\": [\n",
      "        {\n",
      "          \"bytes\": 30646,\n",
      "          \"created_at\": 1680670900,\n",
      "          \"filename\": \"compiled_results.csv\",\n",
      "          \"id\": \"file-84jsgYpEmfSSD7c01QISDUm5\",\n",
      "          \"object\": \"file\",\n",
      "          \"purpose\": \"fine-tune-results\",\n",
      "          \"status\": \"processed\",\n",
      "          \"status_details\": null\n",
      "        }\n",
      "      ],\n",
      "      \"status\": \"succeeded\",\n",
      "      \"training_files\": [\n",
      "        {\n",
      "          \"bytes\": 127998,\n",
      "          \"created_at\": 1680670676,\n",
      "          \"filename\": \"data/prepared_data_more_prepared.jsonl\",\n",
      "          \"id\": \"file-Koi8mPKvDINOnBnI03J9iXnS\",\n",
      "          \"object\": \"file\",\n",
      "          \"purpose\": \"fine-tune\",\n",
      "          \"status\": \"processed\",\n",
      "          \"status_details\": null\n",
      "        }\n",
      "      ],\n",
      "      \"updated_at\": 1680670900,\n",
      "      \"validation_files\": []\n",
      "    }\n",
      "  ],\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['openai', 'api', 'fine_tunes.list'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run('openai api fine_tunes.list'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "宋朝时期，有一位英雄人物叫做李英雄，他拥有超凡的武功，曾经参加过多次战斗，拯救了无数的人民。\n",
      "\n",
      "一次，李英雄突然发现自己穿越到了现代，他发现自己的超能力比起宋朝时期还要强大，他可以发射激光，可以控制天空，可以控制空气，甚至可以控制大地。\n",
      "\n",
      "李英雄发现，现代的世界正面临着一个可怕的威胁——怪兽，他们正在毁灭人类，李英雄决定要去拯救世界，于是他和奥特曼一起出发，开始了一场艰苦的战斗。\n",
      "\n",
      "李英雄凭借着自己的超能力，和奥特曼一起，终于战胜了怪兽，拯救了世界。他以英雄的姿态拥有了榜样，成为了人们心中的英雄。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def write_a_story(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"curie:ft-bothub-ai:ultraman-2023-04-04-03-03-26\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=2000,\n",
    "        top_p=1,\n",
    "        stop=[\".\"])\n",
    "    return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "story = write_a_story(\"宋,发射激光,艰难 ->\\n\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "这是一个关于一位叫做林黛玉的英雄人物的传奇故事。林黛玉曾经是一个英勇的将军，他在古代早期就受到了神灵们的授予，被封为“神童”。\n",
      "\n",
      "一次，神灵们派遣一群令人鼻颈的怪兽出征，攻击辛苦的中原人民。林黛玉毫不犹豫地指挥令人恐惧的大军，抵抗怪兽的攻击，为人民谋幸福安乐。\n",
      "\n",
      "但是，神灵们的力量太强大了，林黛玉不得不放弃了自己的家乡，离开了家园，穿越到了现代，并获得了一种超能力——龙卷风。\n",
      "\n",
      "林黛玉很快就感受到了现代的威力，他发现自己可以用龙卷风来抵抗怪兽，于是他决定和奥特曼一起，把怪兽一个个击败，拯救人类。\n",
      "\n",
      "他们在一次次的激烈战斗中，最终击败了最强大的怪兽，拯救了人类，林黛玉也回到了家乡，成为了一个英雄。\n",
      "\n",
      "林黛玉的传奇故事，令人惊叹，他的勇敢和智慧，也令人敬佩。他的故事，让人们永远不会忘记。\n"
     ]
    }
   ],
   "source": [
    "story = write_a_story(\"秦,龙卷风,辛苦 ->\\n\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x)) # subtract max(x) for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "s = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "print(softmax(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "## 注意，s1和s2我们是不存储的，是QV的计算结果\n",
    "s1 = np.array([0.1, 0.2])\n",
    "s2 = np.array([0.3, 0.4])\n",
    "\n",
    "## 我们只存s1和s2\n",
    "m1 = np.max(s1)\n",
    "m2 = np.max(s2)\n",
    "\n",
    "print(m1)\n",
    "print(m2)\n",
    "\n",
    "m = np.max([m1, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90483742 1.        ]\n",
      "[0.90483742 1.        ]\n"
     ]
    }
   ],
   "source": [
    "fx1 = np.exp(s1 - m1)\n",
    "fx2 = np.exp(s2 - m2)\n",
    "\n",
    "print(fx1)\n",
    "print(fx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74081822 0.81873075 0.90483742 1.        ]\n"
     ]
    }
   ],
   "source": [
    "fx = [np.exp(m1-m) * np.exp(s1 - m1), np.exp(m2-m) * np.exp(s2 - m2)]\n",
    "print(np.hstack(fx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.464386391795659\n"
     ]
    }
   ],
   "source": [
    "lx = np.exp(m1 - m) * np.sum(np.exp(s1 - m1)) + np.exp(m2 - m) * np.sum(np.exp(s2 - m2))\n",
    "print(lx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "result = np.hstack(fx)/lx\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenzier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[1951, 68, 698, 3256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "text = \"Deeplearning\"\n",
    "print(len(encoding.encode(text)))\n",
    "print(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De\n",
      "e\n",
      "ple\n",
      "arning\n"
     ]
    }
   ],
   "source": [
    "for id in encoding.encode(text):\n",
    "    print(encoding.decode([id]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩充中文词表的Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 20000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "32000\n",
      "Before:32000\n",
      "New model pieces: 49953\n",
      "Chinese-LLaMA tokenizer has been saved to merged_tokenizer_hf\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Test text:\n",
      " 白日依山尽，黄河入海流。欲穷千里目，更上一层楼。\n",
      "The primary use of LLaMA is research on large language models, including\n",
      "Tokenized by LLaMA tokenizer:['▁', '白', '日', '<0xE4>', '<0xBE>', '<0x9D>', '山', '<0xE5>', '<0xB0>', '<0xBD>', '，', '黄', '河', '入', '海', '流', '。', '<0xE6>', '<0xAC>', '<0xB2>', '<0xE7>', '<0xA9>', '<0xB7>', '千', '里', '目', '，', '更', '上', '一', '<0xE5>', '<0xB1>', '<0x82>', '<0xE6>', '<0xA5>', '<0xBC>', '。', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n",
      "Tokenized by Chinese-LLaMA tokenizer:['▁白', '日', '依', '山', '尽', '，', '黄河', '入', '海', '流', '。', '欲', '穷', '千里', '目', '，', '更', '上', '一层', '楼', '。', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "\n",
    "llama_tokenizer_dir = \"./data/llama_tokenizer\"\n",
    "chinese_sp_model_file = \"./data/chinese_sp_model/chinese_sp.model\"\n",
    "\n",
    "# load\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_sp_model = spm.SentencePieceProcessor()\n",
    "chinese_sp_model.Load(chinese_sp_model_file)\n",
    "\n",
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n",
    "\n",
    "# print number of tokens\n",
    "print(len(llama_tokenizer),len(chinese_sp_model))\n",
    "print(llama_tokenizer.all_special_tokens)\n",
    "print(llama_tokenizer.all_special_ids)\n",
    "print(llama_tokenizer.special_tokens_map)\n",
    "\n",
    "## Add Chinese tokens to LLaMA tokenizer\n",
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "print(len(llama_spm_tokens_set))\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n",
    "\n",
    "## Save\n",
    "output_sp_dir = 'merged_tokenizer_sp'\n",
    "output_hf_dir = 'merged_tokenizer_hf' # the path to save Chinese-LLaMA tokenizer\n",
    "os.makedirs(output_sp_dir,exist_ok=True)\n",
    "with open(output_sp_dir+'/chinese_llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir+'/chinese_llama.model')\n",
    "\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")\n",
    "\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
    "print(chinese_llama_tokenizer.all_special_tokens)\n",
    "print(chinese_llama_tokenizer.all_special_ids)\n",
    "print(chinese_llama_tokenizer.special_tokens_map)\n",
    "text='''白日依山尽，黄河入海流。欲穷千里目，更上一层楼。\n",
    "The primary use of LLaMA is research on large language models, including'''\n",
    "print(\"Test text:\\n\",text)\n",
    "print\n",
    "print(f\"Tokenized by LLaMA tokenizer:{llama_tokenizer.tokenize(text)}\")\n",
    "print(f\"Tokenized by Chinese-LLaMA tokenizer:{chinese_llama_tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用SentencePiece进行Tokenizer的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '你', '好', ',', '世界', '!', '一个', '美', '好', '的', '世界']\n",
      "你好,世界!一个美好的世界\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./data/mr_fujino/mr_fujino.txt --model_prefix=m --vocab_size=2000 --model_type=bpe\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/mr_fujino/mr_fujino.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: ./data/mr_fujino/mr_fujino.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 38 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=3266\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9694% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=722\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999694\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 38 sentences.\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 38\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 39\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=15 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=20 all=2856 active=1176 piece=一个\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=40 all=2959 active=1279 piece=研究\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=60 all=3034 active=1354 piece=其时\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=80 all=3109 active=1429 piece=照相\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=100 all=3162 active=1482 piece=”,\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=120 all=3198 active=1037 piece=他是\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=140 all=3229 active=1068 piece=可以\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=160 all=3269 active=1108 piece=开看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=180 all=3311 active=1150 piece=有些\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=200 all=3355 active=1194 piece=说道\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=220 all=3391 active=1035 piece=学生会\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=240 all=3430 active=1074 piece=虽然觉\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=260 all=3459 active=1103 piece=”。\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=280 all=3469 active=1113 piece=不息\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=300 all=3485 active=1129 piece=也颇\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=320 all=3499 active=1013 piece=从此\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=340 all=3510 active=1024 piece=便连\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=360 all=3521 active=1035 piece=到会\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=380 all=3534 active=1048 piece=发达\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=400 all=3547 active=1061 piece=围着\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=420 all=3560 active=1013 piece=外套\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=440 all=3571 active=1024 piece=寒颤\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=460 all=3585 active=1038 piece=平的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=480 all=3597 active=1050 piece=息了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=500 all=3602 active=1055 piece=据说\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=520 all=3613 active=1011 piece=尘斗\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=540 all=3624 active=1022 piece=日暮\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=560 all=3640 active=1038 piece=本的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=580 all=3655 active=1053 piece=点上\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=600 all=3671 active=1069 piece=的几\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=620 all=3683 active=1011 piece=添教\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=640 all=3699 active=1027 piece=真的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=660 all=3703 active=1031 piece=脱漏\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=680 all=3714 active=1042 piece=谎话\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=700 all=3726 active=1054 piece=问问\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=720 all=3735 active=1009 piece=形成\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=740 all=3748 active=1022 piece=闲看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=760 all=3749 active=1023 piece=一一订\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=780 all=3760 active=1034 piece=东京玩\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=800 all=3767 active=1041 piece=了一封\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=820 all=3775 active=1007 piece=好了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=840 all=3789 active=1021 piece=郎的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=860 all=3798 active=1030 piece=人的人\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=880 all=3804 active=1036 piece=住处了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=900 all=3811 active=1043 piece=兼以满\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=920 all=3818 active=1007 piece=面看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=940 all=3826 active=1015 piece=到一种\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=960 all=3836 active=1025 piece=又因为\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=980 all=3844 active=1033 piece=在内了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1000 all=3850 active=1039 piece=学校去\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1020 all=3855 active=1005 piece=订成\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1040 all=3863 active=1013 piece=寓居的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1060 all=3872 active=1022 piece=往浙江\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1080 all=3877 active=1027 piece=我怎么\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1100 all=3882 active=1032 piece=是说上\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1120 all=3892 active=1009 piece=是骨学\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1140 all=3903 active=1020 piece=样的画\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1160 all=3911 active=1028 piece=的终结\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1180 all=3915 active=1032 piece=自己道\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1200 all=3918 active=1035 piece=进温室\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1220 all=3919 active=1001 piece=一个客店\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1240 all=3928 active=1010 piece=东京出发\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1260 all=3936 active=1018 piece=于是点上\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train('--input=./data/mr_fujino/mr_fujino.txt --model_prefix=m --vocab_size=2000 --model_type=bpe')\n",
    "\n",
    "# Load SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "# Test encode and decode\n",
    "encoded = sp.encode_as_pieces('你好，世界！一个美好的世界')\n",
    "print(encoded)\n",
    "decoded = sp.decode_pieces(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mactalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
