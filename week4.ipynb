{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flash Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x)) # subtract max(x) for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "s = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "print(softmax(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "## 注意，s1和s2我们是不存储的，是QV的计算结果\n",
    "s1 = np.array([0.1, 0.2])\n",
    "s2 = np.array([0.3, 0.4])\n",
    "\n",
    "m1 = np.max(s1)\n",
    "m2 = np.max(s2)\n",
    "\n",
    "print(m1)\n",
    "print(m2)\n",
    "\n",
    "m = np.max([m1, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90483742 1.        ]\n",
      "[0.90483742 1.        ]\n"
     ]
    }
   ],
   "source": [
    "fx1 = np.exp(s1 - m1)\n",
    "fx2 = np.exp(s2 - m2)\n",
    "\n",
    "print(fx1)\n",
    "print(fx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74081822 0.81873075 0.90483742 1.        ]\n"
     ]
    }
   ],
   "source": [
    "fx = [np.exp(m1-m) * np.exp(s1 - m1), np.exp(m2-m) * np.exp(s2 - m2)]\n",
    "print(np.hstack(fx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.464386391795659\n"
     ]
    }
   ],
   "source": [
    "lx = np.exp(m1 - m) * np.sum(np.exp(s1 - m1)) + np.exp(m2 - m) * np.sum(np.exp(s2 - m2))\n",
    "print(lx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "result = np.hstack(fx)/lx\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[1951, 68, 698, 3256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "text = \"Deeplearning\"\n",
    "print(len(encoding.encode(text)))\n",
    "print(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De\n",
      "e\n",
      "ple\n",
      "arning\n"
     ]
    }
   ],
   "source": [
    "for id in encoding.encode(text):\n",
    "    print(encoding.decode([id]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩充中文词表的Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 20000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "32000\n",
      "Before:32000\n",
      "New model pieces: 49953\n",
      "Chinese-LLaMA tokenizer has been saved to merged_tokenizer_hf\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Test text:\n",
      " 白日依山尽，黄河入海流。欲穷千里目，更上一层楼。\n",
      "The primary use of LLaMA is research on large language models, including\n",
      "Tokenized by LLaMA tokenizer:['▁', '白', '日', '<0xE4>', '<0xBE>', '<0x9D>', '山', '<0xE5>', '<0xB0>', '<0xBD>', '，', '黄', '河', '入', '海', '流', '。', '<0xE6>', '<0xAC>', '<0xB2>', '<0xE7>', '<0xA9>', '<0xB7>', '千', '里', '目', '，', '更', '上', '一', '<0xE5>', '<0xB1>', '<0x82>', '<0xE6>', '<0xA5>', '<0xBC>', '。', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n",
      "Tokenized by Chinese-LLaMA tokenizer:['▁白', '日', '依', '山', '尽', '，', '黄河', '入', '海', '流', '。', '欲', '穷', '千里', '目', '，', '更', '上', '一层', '楼', '。', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "\n",
    "llama_tokenizer_dir = \"./data/llama_tokenizer\"\n",
    "chinese_sp_model_file = \"./data/chinese_sp_model/chinese_sp.model\"\n",
    "\n",
    "# load\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_sp_model = spm.SentencePieceProcessor()\n",
    "chinese_sp_model.Load(chinese_sp_model_file)\n",
    "\n",
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n",
    "\n",
    "# print number of tokens\n",
    "print(len(llama_tokenizer),len(chinese_sp_model))\n",
    "print(llama_tokenizer.all_special_tokens)\n",
    "print(llama_tokenizer.all_special_ids)\n",
    "print(llama_tokenizer.special_tokens_map)\n",
    "\n",
    "## Add Chinese tokens to LLaMA tokenizer\n",
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "print(len(llama_spm_tokens_set))\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n",
    "\n",
    "## Save\n",
    "output_sp_dir = 'merged_tokenizer_sp'\n",
    "output_hf_dir = 'merged_tokenizer_hf' # the path to save Chinese-LLaMA tokenizer\n",
    "os.makedirs(output_sp_dir,exist_ok=True)\n",
    "with open(output_sp_dir+'/chinese_llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir+'/chinese_llama.model')\n",
    "\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")\n",
    "\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
    "print(chinese_llama_tokenizer.all_special_tokens)\n",
    "print(chinese_llama_tokenizer.all_special_ids)\n",
    "print(chinese_llama_tokenizer.special_tokens_map)\n",
    "text='''白日依山尽，黄河入海流。欲穷千里目，更上一层楼。\n",
    "The primary use of LLaMA is research on large language models, including'''\n",
    "print(\"Test text:\\n\",text)\n",
    "print\n",
    "print(f\"Tokenized by LLaMA tokenizer:{llama_tokenizer.tokenize(text)}\")\n",
    "print(f\"Tokenized by Chinese-LLaMA tokenizer:{chinese_llama_tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用SentencePiece进行Tokenizer的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '你', '好', ',', '世界', '!', '一个', '美', '好', '的', '世界']\n",
      "你好,世界!一个美好的世界\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./data/mr_fujino/mr_fujino.txt --model_prefix=m --vocab_size=2000 --model_type=bpe\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/mr_fujino/mr_fujino.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: ./data/mr_fujino/mr_fujino.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 38 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=3266\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9694% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=722\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999694\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 38 sentences.\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 38\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 39\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=15 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=20 all=2856 active=1176 piece=一个\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=40 all=2959 active=1279 piece=研究\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=60 all=3034 active=1354 piece=其时\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=80 all=3109 active=1429 piece=照相\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=100 all=3162 active=1482 piece=”,\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=120 all=3198 active=1037 piece=他是\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=140 all=3229 active=1068 piece=可以\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=160 all=3269 active=1108 piece=开看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=180 all=3311 active=1150 piece=有些\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=200 all=3355 active=1194 piece=说道\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=220 all=3391 active=1035 piece=学生会\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=240 all=3430 active=1074 piece=虽然觉\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=260 all=3459 active=1103 piece=”。\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=280 all=3469 active=1113 piece=不息\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=300 all=3485 active=1129 piece=也颇\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=320 all=3499 active=1013 piece=从此\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=340 all=3510 active=1024 piece=便连\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=360 all=3521 active=1035 piece=到会\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=380 all=3534 active=1048 piece=发达\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=400 all=3547 active=1061 piece=围着\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=420 all=3560 active=1013 piece=外套\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=440 all=3571 active=1024 piece=寒颤\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=460 all=3585 active=1038 piece=平的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=480 all=3597 active=1050 piece=息了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=500 all=3602 active=1055 piece=据说\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=520 all=3613 active=1011 piece=尘斗\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=540 all=3624 active=1022 piece=日暮\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=560 all=3640 active=1038 piece=本的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=580 all=3655 active=1053 piece=点上\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=600 all=3671 active=1069 piece=的几\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=620 all=3683 active=1011 piece=添教\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=640 all=3699 active=1027 piece=真的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=660 all=3703 active=1031 piece=脱漏\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=680 all=3714 active=1042 piece=谎话\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=700 all=3726 active=1054 piece=问问\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=720 all=3735 active=1009 piece=形成\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=740 all=3748 active=1022 piece=闲看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=760 all=3749 active=1023 piece=一一订\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=780 all=3760 active=1034 piece=东京玩\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=800 all=3767 active=1041 piece=了一封\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=820 all=3775 active=1007 piece=好了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=840 all=3789 active=1021 piece=郎的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=860 all=3798 active=1030 piece=人的人\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=880 all=3804 active=1036 piece=住处了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=900 all=3811 active=1043 piece=兼以满\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=920 all=3818 active=1007 piece=面看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=940 all=3826 active=1015 piece=到一种\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=960 all=3836 active=1025 piece=又因为\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=980 all=3844 active=1033 piece=在内了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1000 all=3850 active=1039 piece=学校去\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1020 all=3855 active=1005 piece=订成\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1040 all=3863 active=1013 piece=寓居的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1060 all=3872 active=1022 piece=往浙江\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1080 all=3877 active=1027 piece=我怎么\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1100 all=3882 active=1032 piece=是说上\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1120 all=3892 active=1009 piece=是骨学\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1140 all=3903 active=1020 piece=样的画\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1160 all=3911 active=1028 piece=的终结\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1180 all=3915 active=1032 piece=自己道\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1200 all=3918 active=1035 piece=进温室\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1220 all=3919 active=1001 piece=一个客店\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1240 all=3928 active=1010 piece=东京出发\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1260 all=3936 active=1018 piece=于是点上\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train('--input=./data/mr_fujino/mr_fujino.txt --model_prefix=m --vocab_size=2000 --model_type=bpe')\n",
    "\n",
    "# Load SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "# Test encode and decode\n",
    "encoded = sp.encode_as_pieces('你好，世界！一个美好的世界')\n",
    "print(encoded)\n",
    "decoded = sp.decode_pieces(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mactalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
